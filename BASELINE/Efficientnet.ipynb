{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2bcba5f-002e-4f49-9622-ada6117faf0a",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61ad9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install albumentations ;!pip3 install opencv-python ; !pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23a79e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manthoniy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0f89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9mhdfy1m) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">spring-moon-11</strong> at: <a href='https://wandb.ai/anthoniy/Dacon-papering-competition/runs/9mhdfy1m' target=\"_blank\">https://wandb.ai/anthoniy/Dacon-papering-competition/runs/9mhdfy1m</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>c:\\Users\\DoSungjin\\Documents\\GitHub\\Dacon_papering_classification\\BASELINE\\wandb\\run-20230512_004633-9mhdfy1m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9mhdfy1m). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\DoSungjin\\Documents\\GitHub\\Dacon_papering_classification\\BASELINE\\wandb\\run-20230512_111951-kvgaiy7e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anthoniy/Dacon-papering-competition/runs/kvgaiy7e' target=\"_blank\">flowing-pyramid-12</a></strong> to <a href='https://wandb.ai/anthoniy/Dacon-papering-competition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anthoniy/Dacon-papering-competition' target=\"_blank\">https://wandb.ai/anthoniy/Dacon-papering-competition</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anthoniy/Dacon-papering-competition/runs/kvgaiy7e' target=\"_blank\">https://wandb.ai/anthoniy/Dacon-papering-competition/runs/kvgaiy7e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/anthoniy/Dacon-papering-competition/runs/kvgaiy7e?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2a7fae0b6d0>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Dacon-papering-competition\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    'IMG_SIZE':224,\n",
    "    \"architecture\": \"Efficientnet-b0\",\n",
    "    'EPOCHS':30,\n",
    "    'SEED':42,\n",
    "    'BATCH_SIZE':64,\n",
    "    \"dataset\": \"Dacon-papering-competition\",\n",
    "    }# hyperparameter\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e9f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LEARNING_RATE': 0.003, 'IMG_SIZE': 224, 'architecture': 'Efficientnet-b0', 'EPOCHS': 50, 'SEED': 42, 'BATCH_SIZE': 32, 'dataset': 'Dacon-papering-competition'}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameter\n",
    "CFG = wandb.config\n",
    "CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d9b68-7102-4eca-9543-3b9b8acafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# torch.multiprocessing import\n",
    "from torch import multiprocessing\n",
    "from collections import OrderedDict, Counter\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9614e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\DoSungjin\\\\Documents\\\\GitHub\\\\Dacon_papering_classification\\\\DATA'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 경로지정\n",
    "import os\n",
    "os.chdir('../DATA')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13862e3-bb27-47af-9b58-a9fbf804df71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Set the device to CPU or GPU depending on availability\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4254e860-ff82-43ba-bfa3-fcee4eb3ddbd",
   "metadata": {},
   "source": [
    "## Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a714b-71b6-4475-a4ce-fa5f98bc2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05a4172e-5791-446f-9616-35c09d8bf25a",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abecd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\DoSungjin\\\\Documents\\\\GitHub\\\\Dacon_papering_classification\\\\DATA'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 데이터셋 디렉토리 경로\n",
    "dataset_dir = \"new_train\"\n",
    "\n",
    "# 모든 이미지 파일 경로 리스트\n",
    "all_img_list = []\n",
    "folder_list = []\n",
    "train_file_list = os.listdir(dataset_dir)\n",
    "for item in train_file_list:\n",
    "    item_path = os.path.join(dataset_dir, item)\n",
    "    for file in os.listdir(item_path):\n",
    "        all_img_list.append(os.path.join(item_path, file))\n",
    "        folder_list.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68163566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # folder name list\n",
    "ori_names = os.listdir('ori_train')\n",
    "\n",
    "new_names = ['furniture_repair', 'cleaning_mop_holder_repair', 'mold', 'twist', 'rust_contamination', 'wobbling', 'fabric_defect', 'molding_repair', 'stain', 'plaster_repair', 'pollution', 'typographical_error', 'crying', 'defective_joint', 'windowdoor_frame_repair', 'cracking', 'excessive_gap', 'piece', 'damage']\n",
    "\n",
    "# print(len(ori_names), len(new_names))\n",
    "# os.chdir('new_train')\n",
    "# # # folder rename\n",
    "# for ori_name, new_name in zip(ori_names, new_names):\n",
    "#     os.rename(ori_name, new_name)\n",
    "\n",
    "# print(os.listdir())\n",
    "# os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d43a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n",
      "go on\n"
     ]
    }
   ],
   "source": [
    "for a,b in zip(ori_names, new_names):\n",
    "    len_a = len(os.listdir(f'ori_train/{a}'))\n",
    "    len_b = len(os.listdir(f'new_train/{b}'))\n",
    "    if len_a == len_b:\n",
    "        print('go on')\n",
    "    else: \n",
    "        print('stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed76bfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DoSungjin\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# If you are using CUDA\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4119733d-adef-436c-afca-4112a9225d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['img_path', 'label'])\n",
    "df['img_path'] = all_img_list\n",
    "df['label'] = df['img_path'].apply(lambda x : str(x).split('\\\\')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7bcf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'cleaning_mop_holder_repair': 307,\n",
       "         'cracking': 162,\n",
       "         'crying': 22,\n",
       "         'damage': 1405,\n",
       "         'defective_joint': 17,\n",
       "         'excessive_gap': 5,\n",
       "         'fabric_defect': 99,\n",
       "         'furniture_repair': 12,\n",
       "         'mold': 145,\n",
       "         'molding_repair': 130,\n",
       "         'piece': 51,\n",
       "         'plaster_repair': 57,\n",
       "         'pollution': 595,\n",
       "         'rust_contamination': 14,\n",
       "         'stain': 3,\n",
       "         'twist': 210,\n",
       "         'typographical_error': 142,\n",
       "         'windowdoor_frame_repair': 27,\n",
       "         'wobbling': 54})"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7e87bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100개 이하 그룹: ['crying', 'defective_joint', 'excessive_gap', 'fabric_defect', 'furniture_repair', 'piece', 'plaster_repair', 'rust_contamination', 'stain', 'windowdoor_frame_repair', 'wobbling']\n",
      "100~600개 그룹: ['cleaning_mop_holder_repair', 'cracking', 'mold', 'molding_repair', 'pollution', 'twist', 'typographical_error']\n",
      "600개 이상 그룹: ['damage']\n"
     ]
    }
   ],
   "source": [
    "group_100_or_less = []\n",
    "group_100_to_600 = []\n",
    "group_600_or_more = []\n",
    "\n",
    "for key, value in Counter(df['label']).items():\n",
    "    if value <= 100:\n",
    "        group_100_or_less.append(key)\n",
    "    elif value <= 600:\n",
    "        group_100_to_600.append(key)\n",
    "    else:\n",
    "        group_600_or_more.append(key)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"100개 이하 그룹:\", group_100_or_less)\n",
    "print(\"100~600개 그룹:\", group_100_to_600)\n",
    "print(\"600개 이상 그룹:\", group_600_or_more)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb63d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# 100개 이하 그룹 오버샘플링\n",
    "minor_class_samples_100_or_less = df[df['label'].isin(group_100_or_less)]\n",
    "minor_class_count_100_or_less = len(minor_class_samples_100_or_less)\n",
    "oversample_count_100_or_less = int(minor_class_count_100_or_less * 5)\n",
    "oversampled_minor_class_100_or_less = resample(minor_class_samples_100_or_less, replace=True, n_samples=oversample_count_100_or_less, random_state=42)\n",
    "\n",
    "# 100~600개 그룹 오버샘플링\n",
    "minor_class_samples_100_to_600 = df[df['label'].isin(group_100_to_600)]\n",
    "minor_class_count_100_to_600 = len(minor_class_samples_100_to_600)\n",
    "oversample_count_100_to_600 = int(minor_class_count_100_to_600 * 1.5)\n",
    "oversampled_minor_class_100_to_600 = resample(minor_class_samples_100_to_600, replace=True, n_samples=oversample_count_100_to_600, random_state=42)\n",
    "\n",
    "# 600개 이상 그룹 오버샘플링\n",
    "minor_class_samples_600_or_more = df[df['label'].isin(group_600_or_more)]\n",
    "oversampled_minor_class_600_or_more = minor_class_samples_600_or_more  # 그대로 유지\n",
    "\n",
    "# 오버샘플링된 소수 클래스 데이터셋과 다수 클래스 데이터셋 합치기\n",
    "df_oversampled = pd.concat([oversampled_minor_class_100_or_less, oversampled_minor_class_100_to_600, oversampled_minor_class_600_or_more])\n",
    "\n",
    "# 샘플 순서 랜덤하게 섞기\n",
    "df_oversampled = df_oversampled.sample(frac=1, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db41c93-3515-4fcd-936b-0a01f5388b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val, _, _ = train_test_split(df_oversampled, df_oversampled['label'], test_size=0.2, stratify=df_oversampled['label'], random_state=CFG['SEED'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb1f117e-105d-4e9e-b9bd-938d4271a940",
   "metadata": {},
   "source": [
    "## Label-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8c5916-8065-4b5c-aa37-f3fb2b9fa422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3584</th>\n",
       "      <td>new_train\\pollution\\388.png</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4431</th>\n",
       "      <td>new_train\\damage\\842.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>new_train\\mold\\9.png</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>new_train\\crying\\2.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>new_train\\fabric_defect\\72.png</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2720</th>\n",
       "      <td>new_train\\piece\\33.png</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>new_train\\damage\\520.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>new_train\\typographical_error\\99.png</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3444</th>\n",
       "      <td>new_train\\wobbling\\17.png</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>new_train\\mold\\73.png</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4596 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  img_path  label\n",
       "3584           new_train\\pollution\\388.png     12\n",
       "4431              new_train\\damage\\842.png      3\n",
       "2227                  new_train\\mold\\9.png      8\n",
       "1164                new_train\\crying\\2.png      2\n",
       "1171        new_train\\fabric_defect\\72.png      6\n",
       "...                                    ...    ...\n",
       "2720                new_train\\piece\\33.png     10\n",
       "5319              new_train\\damage\\520.png      3\n",
       "5280  new_train\\typographical_error\\99.png     16\n",
       "3444             new_train\\wobbling\\17.png     18\n",
       "5741                 new_train\\mold\\73.png      8\n",
       "\n",
       "[4596 rows x 2 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "train['label'] = le.fit_transform(train['label'])\n",
    "val['label'] = le.transform(val['label'])\n",
    "train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac27ed36-8031-47a7-bd0d-a913513f2e8e",
   "metadata": {},
   "source": [
    "## CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd60a5-24e2-4539-bfd0-1c374a641699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, img_path_list, label_list, transforms=None):\n",
    "#         self.img_path_list = img_path_list\n",
    "#         self.label_list = label_list\n",
    "#         self.transforms = transforms\n",
    "        \n",
    "#     def __getitem__(self, index):\n",
    "#         img_path = self.img_path_list[index]\n",
    "        \n",
    "#         image = cv2.imread(img_path)\n",
    "        \n",
    "#         if self.transforms is not None:\n",
    "#             image = self.transforms(image=image)['image']\n",
    "        \n",
    "#         if self.label_list is not None:\n",
    "#             label = self.label_list[index]\n",
    "#             return image, label\n",
    "#         else:\n",
    "#             return image\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340b4a8b-5d6c-413f-b8b6-066e91a660e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import albumentations as A\n",
    "# from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "# class_aware_augmentation_minor = A.Compose([\n",
    "#     A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.RandomBrightnessContrast(p=0.2),\n",
    "#     A.OneOf([\n",
    "#         A.Rotate(limit=10),\n",
    "#         A.IAAAffine(rotate=10)\n",
    "#     ], p=0.5),\n",
    "#     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "#     ToTensorV2(),\n",
    "# ])\n",
    "\n",
    "# general_augmentation_major = A.Compose([\n",
    "#     A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.Rotate(limit=10, p=0.5),\n",
    "#     A.RandomBrightnessContrast(p=0.2),\n",
    "#     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "#     ToTensorV2(),\n",
    "# ])\n",
    "\n",
    "# def apply_augmentation(image, label):\n",
    "#     if label in keys_under_100:\n",
    "#         augmented = class_aware_augmentation_minor(image=image)\n",
    "#     else:\n",
    "#         augmented = general_augmentation_major(image=image)\n",
    "#     return augmented['image'], label\n",
    "\n",
    "# test_transform = A.Compose([\n",
    "#     A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "#     A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "#     ToTensorV2()\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eddc330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3584</th>\n",
       "      <td>new_train\\pollution\\388.png</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4431</th>\n",
       "      <td>new_train\\damage\\842.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>new_train\\mold\\9.png</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>new_train\\crying\\2.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>new_train\\fabric_defect\\72.png</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2720</th>\n",
       "      <td>new_train\\piece\\33.png</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>new_train\\damage\\520.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>new_train\\typographical_error\\99.png</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3444</th>\n",
       "      <td>new_train\\wobbling\\17.png</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>new_train\\mold\\73.png</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4596 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  img_path  label\n",
       "3584           new_train\\pollution\\388.png     12\n",
       "4431              new_train\\damage\\842.png      3\n",
       "2227                  new_train\\mold\\9.png      8\n",
       "1164                new_train\\crying\\2.png      2\n",
       "1171        new_train\\fabric_defect\\72.png      6\n",
       "...                                    ...    ...\n",
       "2720                new_train\\piece\\33.png     10\n",
       "5319              new_train\\damage\\520.png      3\n",
       "5280  new_train\\typographical_error\\99.png     16\n",
       "3444             new_train\\wobbling\\17.png     18\n",
       "5741                 new_train\\mold\\73.png      8\n",
       "\n",
       "[4596 rows x 2 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4892d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\DoSungjin\\\\Documents\\\\GitHub\\\\Dacon_papering_classification\\\\DATA'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f043f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['new_train\\\\pollution\\\\388.png', 'new_train\\\\damage\\\\842.png',\n",
       "       'new_train\\\\mold\\\\9.png', ...,\n",
       "       'new_train\\\\typographical_error\\\\99.png',\n",
       "       'new_train\\\\wobbling\\\\17.png', 'new_train\\\\mold\\\\73.png'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['img_path'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d880481-1965-499d-9caa-fdfa8526f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# class ClassAwareAugmentationMinor:\n",
    "#     def __init__(self, img_size):\n",
    "#         self.transform = A.Compose([\n",
    "#             A.Resize(img_size, img_size),\n",
    "#             A.HorizontalFlip(p=0.5),\n",
    "#             A.OneOf([\n",
    "#                 A.Rotate(limit=10),\n",
    "#                 A.IAAAffine(rotate=10)\n",
    "#             ], p=0.5),\n",
    "#             A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "#             ToTensorV2(),\n",
    "#         ])\n",
    "\n",
    "#     def __call__(self, image):\n",
    "#         augmented = self.transform(image=image)\n",
    "#         return augmented['image']\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=10, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    ToTensorV2() \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "class CustomDataset:\n",
    "    def __init__(self, img_paths, labels, train_transform): #, transform_minor):\n",
    "        self.img_paths = img_paths\n",
    "        self.labels = labels\n",
    "        self.train_transform = train_transform\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = train_transform(image=image)['image']\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(train['img_path'].values, train['label'].values, train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val['img_path'].values, val['label'].values, test_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39962463-032f-490a-a76d-c03991795f38",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664c4d0-f1f2-4971-9090-4d6ee66309ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes=len(le.classes_)):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = models.efficientnet_b0(pretrained=True)\n",
    "        self.classifier = nn.Linear(1000, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10e166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15, dtype=torch.int32)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[1][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "122af0aa-a1fd-4595-9488-35761e3cb596",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a17df6b3-16c9-44dd-b0fd-ffb501fee749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for imgs, labels in tqdm(iter(train_loader)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            label_map = {label: i for i, label in enumerate(set(labels.tolist()))}\n",
    "            numerical_labels = [label_map[int(label.item())] for label in labels]\n",
    "            labels = torch.tensor(numerical_labels, dtype=torch.long)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(imgs)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val Weighted F1 Score : [{_val_score:.5f}]')\n",
    "        # wandb에 로그 기록\n",
    "        wandb.log({'epoch': epoch, 'train_loss': _train_loss, 'val_loss': _val_loss, 'val_score': _val_score})\n",
    "        \n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val Weighted F1 Score : [{_val_score:.5f}]')\n",
    "    \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_score)\n",
    "            \n",
    "        if best_score < _val_score:\n",
    "            best_score = _val_score\n",
    "            best_model = model\n",
    "\n",
    "        # wandb에 최적의 모델 저장\n",
    "    wandb.save('best_model.pth')\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "96b1c524-89fb-4ce8-a49f-067fd489f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    preds, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(iter(val_loader)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            label_map = {label: i for i, label in enumerate(set(labels.tolist()))}\n",
    "            numerical_labels = [label_map[int(label.item())] for label in labels]\n",
    "            labels = torch.tensor(numerical_labels, dtype=torch.long)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "            pred = model(imgs)\n",
    "            \n",
    "            loss = criterion(pred, labels)\n",
    "            \n",
    "            preds += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += labels.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "        _val_score = f1_score(true_labels, preds, average='weighted')\n",
    "    \n",
    "    return _val_loss, _val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ca85e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51da39f9-904f-4abd-a7d2-cdf29c4a6c24",
   "metadata": {},
   "source": [
    "## Run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "86142d9a-68b7-4d04-8423-49d28025411d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:48<00:00,  2.97it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [2.62615] Val Loss : [2.50278] Val Weighted F1 Score : [0.12427]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:48<00:00,  2.98it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [2.25091] Val Loss : [2.18359] Val Weighted F1 Score : [0.20064]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:48<00:00,  2.99it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [2.14500] Val Loss : [2.09163] Val Weighted F1 Score : [0.22335]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:48<00:00,  2.99it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [2.11074] Val Loss : [2.23134] Val Weighted F1 Score : [0.16295]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:48<00:00,  2.99it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [2.03714] Val Loss : [2.01822] Val Weighted F1 Score : [0.21385]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:48<00:00,  2.97it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Train Loss : [1.99041] Val Loss : [2.02617] Val Weighted F1 Score : [0.26032]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:49<00:00,  2.92it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Train Loss : [1.89893] Val Loss : [2.06768] Val Weighted F1 Score : [0.21685]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:50<00:00,  2.87it/s]\n",
      "100%|██████████| 36/36 [00:12<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Train Loss : [1.87698] Val Loss : [2.05100] Val Weighted F1 Score : [0.23993]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:50<00:00,  2.83it/s]\n",
      "100%|██████████| 36/36 [00:11<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], Train Loss : [1.86764] Val Loss : [1.84202] Val Weighted F1 Score : [0.29272]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:52<00:00,  2.75it/s]\n",
      "100%|██████████| 36/36 [00:11<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10], Train Loss : [1.79787] Val Loss : [1.79788] Val Weighted F1 Score : [0.31778]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:51<00:00,  2.77it/s]\n",
      "100%|██████████| 36/36 [00:11<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11], Train Loss : [1.75836] Val Loss : [1.91604] Val Weighted F1 Score : [0.28003]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:52<00:00,  2.77it/s]\n",
      "100%|██████████| 36/36 [00:11<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12], Train Loss : [1.71361] Val Loss : [2.00382] Val Weighted F1 Score : [0.26807]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:52<00:00,  2.75it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13], Train Loss : [1.75550] Val Loss : [1.87635] Val Weighted F1 Score : [0.30702]\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.5000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:51<00:00,  2.81it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14], Train Loss : [1.61500] Val Loss : [1.64853] Val Weighted F1 Score : [0.36537]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:48<00:00,  2.98it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15], Train Loss : [1.54826] Val Loss : [1.63648] Val Weighted F1 Score : [0.36450]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:50<00:00,  2.87it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16], Train Loss : [1.54824] Val Loss : [1.72437] Val Weighted F1 Score : [0.33702]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:48<00:00,  2.97it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17], Train Loss : [1.49770] Val Loss : [1.61241] Val Weighted F1 Score : [0.35525]\n",
      "Epoch 00017: reducing learning rate of group 0 to 7.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:48<00:00,  2.96it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18], Train Loss : [1.42300] Val Loss : [1.57909] Val Weighted F1 Score : [0.35909]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:48<00:00,  2.94it/s]\n",
      "100%|██████████| 36/36 [00:11<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19], Train Loss : [1.41580] Val Loss : [1.59620] Val Weighted F1 Score : [0.37168]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:52<00:00,  2.74it/s]\n",
      "100%|██████████| 36/36 [00:11<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20], Train Loss : [1.35595] Val Loss : [1.55870] Val Weighted F1 Score : [0.39613]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:52<00:00,  2.76it/s]\n",
      "100%|██████████| 36/36 [00:11<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21], Train Loss : [1.42271] Val Loss : [1.68683] Val Weighted F1 Score : [0.35240]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:52<00:00,  2.76it/s]\n",
      "100%|██████████| 36/36 [00:11<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22], Train Loss : [1.33736] Val Loss : [1.59059] Val Weighted F1 Score : [0.37625]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:52<00:00,  2.77it/s]\n",
      "100%|██████████| 36/36 [00:11<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23], Train Loss : [1.36819] Val Loss : [1.71085] Val Weighted F1 Score : [0.34908]\n",
      "Epoch 00023: reducing learning rate of group 0 to 3.7500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:52<00:00,  2.76it/s]\n",
      "100%|██████████| 36/36 [00:11<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24], Train Loss : [1.27754] Val Loss : [1.57831] Val Weighted F1 Score : [0.36550]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:51<00:00,  2.79it/s]\n",
      "100%|██████████| 36/36 [00:11<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25], Train Loss : [1.25744] Val Loss : [1.60764] Val Weighted F1 Score : [0.38772]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:52<00:00,  2.76it/s]\n",
      "100%|██████████| 36/36 [00:11<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26], Train Loss : [1.24914] Val Loss : [1.65810] Val Weighted F1 Score : [0.37557]\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.8750e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:51<00:00,  2.77it/s]\n",
      "100%|██████████| 36/36 [00:11<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27], Train Loss : [1.16898] Val Loss : [1.63987] Val Weighted F1 Score : [0.37542]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:52<00:00,  2.76it/s]\n",
      "100%|██████████| 36/36 [00:11<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28], Train Loss : [1.16964] Val Loss : [1.64977] Val Weighted F1 Score : [0.38296]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:52<00:00,  2.77it/s]\n",
      "100%|██████████| 36/36 [00:11<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29], Train Loss : [1.15539] Val Loss : [1.73165] Val Weighted F1 Score : [0.37369]\n",
      "Epoch 00029: reducing learning rate of group 0 to 9.3750e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:50<00:00,  2.83it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30], Train Loss : [1.11739] Val Loss : [1.71280] Val Weighted F1 Score : [0.37396]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:50<00:00,  2.87it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31], Train Loss : [1.15135] Val Loss : [1.70247] Val Weighted F1 Score : [0.37148]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:50<00:00,  2.86it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32], Train Loss : [1.13891] Val Loss : [1.70164] Val Weighted F1 Score : [0.36279]\n",
      "Epoch 00032: reducing learning rate of group 0 to 4.6875e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:50<00:00,  2.86it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33], Train Loss : [1.07365] Val Loss : [1.78903] Val Weighted F1 Score : [0.36535]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:48<00:00,  2.94it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34], Train Loss : [1.07015] Val Loss : [1.78493] Val Weighted F1 Score : [0.38069]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:50<00:00,  2.87it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35], Train Loss : [1.04032] Val Loss : [1.78351] Val Weighted F1 Score : [0.36217]\n",
      "Epoch 00035: reducing learning rate of group 0 to 2.3438e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:51<00:00,  2.78it/s]\n",
      "100%|██████████| 36/36 [00:10<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36], Train Loss : [1.11329] Val Loss : [1.83533] Val Weighted F1 Score : [0.37339]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:49<00:00,  2.92it/s]\n",
      " 31%|███       | 11/36 [00:02<00:06,  3.73it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[252], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdamW(params \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m CFG[\u001b[39m\"\u001b[39m\u001b[39mLEARNING_RATE\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, threshold_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mabs\u001b[39m\u001b[39m'\u001b[39m, min_lr\u001b[39m=\u001b[39m\u001b[39m1e-8\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m infer_model \u001b[39m=\u001b[39m train(model, optimizer, train_loader, val_loader, scheduler, device)\n",
      "Cell \u001b[1;32mIn[250], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_loader, val_loader, scheduler, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     25\u001b[0m     train_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m---> 27\u001b[0m _val_loss, _val_score \u001b[39m=\u001b[39m validation(model, criterion, val_loader, device)\n\u001b[0;32m     28\u001b[0m _train_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(train_loss)\n\u001b[0;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m], Train Loss : [\u001b[39m\u001b[39m{\u001b[39;00m_train_loss\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] Val Loss : [\u001b[39m\u001b[39m{\u001b[39;00m_val_loss\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] Val Weighted F1 Score : [\u001b[39m\u001b[39m{\u001b[39;00m_val_score\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[251], line 7\u001b[0m, in \u001b[0;36mvalidation\u001b[1;34m(model, criterion, val_loader, device)\u001b[0m\n\u001b[0;32m      4\u001b[0m preds, true_labels \u001b[39m=\u001b[39m [], []\n\u001b[0;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m----> 7\u001b[0m     \u001b[39mfor\u001b[39;00m imgs, labels \u001b[39min\u001b[39;00m tqdm(\u001b[39miter\u001b[39m(val_loader)):\n\u001b[0;32m      8\u001b[0m         imgs \u001b[39m=\u001b[39m imgs\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m         label_map \u001b[39m=\u001b[39m {label: i \u001b[39mfor\u001b[39;00m i, label \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mset\u001b[39m(labels\u001b[39m.\u001b[39mtolist()))}\n",
      "File \u001b[1;32mc:\\Users\\DoSungjin\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DoSungjin\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 677\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49mrecord_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_profile_name):\n\u001b[0;32m    678\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m             \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DoSungjin\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\torch\\autograd\\profiler.py:443\u001b[0m, in \u001b[0;36mrecord_function.__init__\u001b[1;34m(self, name, args)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_callbacks_on_exit: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m# Stores underlying RecordFunction as a tensor. TODO: move to custom\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[39m# class (https://github.com/pytorch/pytorch/issues/35026).\u001b[39;00m\n\u001b[1;32m--> 443\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BaseModel()\n",
    "model.eval()\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e275a486-9c59-4b4e-80f6-5000e017b921",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed16d0e-61ee-4737-b90a-3842860cc40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>./test/000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>./test/001.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>./test/002.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>./test/003.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>./test/004.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>TEST_787</td>\n",
       "      <td>./test/787.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>TEST_788</td>\n",
       "      <td>./test/788.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>TEST_789</td>\n",
       "      <td>./test/789.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>TEST_790</td>\n",
       "      <td>./test/790.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>TEST_791</td>\n",
       "      <td>./test/791.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>792 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        img_path\n",
       "0    TEST_000  ./test/000.png\n",
       "1    TEST_001  ./test/001.png\n",
       "2    TEST_002  ./test/002.png\n",
       "3    TEST_003  ./test/003.png\n",
       "4    TEST_004  ./test/004.png\n",
       "..        ...             ...\n",
       "787  TEST_787  ./test/787.png\n",
       "788  TEST_788  ./test/788.png\n",
       "789  TEST_789  ./test/789.png\n",
       "790  TEST_790  ./test/790.png\n",
       "791  TEST_791  ./test/791.png\n",
       "\n",
       "[792 rows x 2 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "82530614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8459"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# If you are using CUDA\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2146a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 12 12:30:44 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.61                 Driver Version: 531.61       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 L...  WDDM | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   40C    P8               10W /  N/A|   5153MiB /  8192MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      5068      C   ...n\\anaconda3\\envs\\gpu-env\\python.exe    N/A      |\n",
      "|    0   N/A  N/A      8652      C   ...n\\anaconda3\\envs\\gpu-env\\python.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbf9ae5-9d8c-4800-a809-63094a1e9a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test['img_path'].values, None, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "378fd3a9-76d8-4c9a-81a1-c6a48492684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in tqdm(iter(test_loader)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            \n",
    "            pred = model(imgs)\n",
    "            \n",
    "            preds += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "    \n",
    "    preds = le.inverse_transform(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2866935c-407d-4919-a58b-1f8feaa66a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:13<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = inference(infer_model, test_loader, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35be0d25-6a06-43bb-bca0-94eda2409a26",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fc864be2-c306-4ad0-aa97-1d5ab5ea9811",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a472198d-6a2f-4d97-b45b-55031d6019a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['label'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eada28d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>damage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>molding_repair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>pollution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>TEST_787</td>\n",
       "      <td>pollution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>TEST_788</td>\n",
       "      <td>cracking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>TEST_789</td>\n",
       "      <td>pollution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>TEST_790</td>\n",
       "      <td>pollution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>TEST_791</td>\n",
       "      <td>damage</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>792 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id           label\n",
       "0    TEST_000          damage\n",
       "1    TEST_001          damage\n",
       "2    TEST_002          damage\n",
       "3    TEST_003  molding_repair\n",
       "4    TEST_004       pollution\n",
       "..        ...             ...\n",
       "787  TEST_787       pollution\n",
       "788  TEST_788        cracking\n",
       "789  TEST_789       pollution\n",
       "790  TEST_790       pollution\n",
       "791  TEST_791          damage\n",
       "\n",
       "[792 rows x 2 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "367e2dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result folder 생성\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "kst = timezone(timedelta(hours=9))\n",
    "train_serial =  datetime.now(kst).strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "Record_path = os.path.join('result', train_serial)\n",
    "\n",
    "os.makedirs(Record_path, exist_ok=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e3af6d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>훼손</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>훼손</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>훼손</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>몰딩수정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>오염</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>TEST_787</td>\n",
       "      <td>오염</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>TEST_788</td>\n",
       "      <td>터짐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>TEST_789</td>\n",
       "      <td>오염</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>TEST_790</td>\n",
       "      <td>오염</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>TEST_791</td>\n",
       "      <td>훼손</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>792 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id label\n",
       "0    TEST_000    훼손\n",
       "1    TEST_001    훼손\n",
       "2    TEST_002    훼손\n",
       "3    TEST_003  몰딩수정\n",
       "4    TEST_004    오염\n",
       "..        ...   ...\n",
       "787  TEST_787    오염\n",
       "788  TEST_788    터짐\n",
       "789  TEST_789    오염\n",
       "790  TEST_790    오염\n",
       "791  TEST_791    훼손\n",
       "\n",
       "[792 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary to map old column names to new column names\n",
    "name_map = dict(zip(new_names, ori_names))\n",
    "name_map\n",
    "\n",
    "\n",
    "# Rename columns using the rename() method\n",
    "submit_test = submit.replace(name_map)\n",
    "submit_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0e3e0511-2ce0-4658-893f-0f3641c50bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_test.to_csv(os.path.join(Record_path,'submission.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "376f22bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'훼손': 375,\n",
       "         '몰딩수정': 27,\n",
       "         '오염': 187,\n",
       "         '오타공': 36,\n",
       "         '터짐': 26,\n",
       "         '걸레받이수정': 18,\n",
       "         '가구수정': 11,\n",
       "         '곰팡이': 23,\n",
       "         '녹오염': 9,\n",
       "         '석고수정': 11,\n",
       "         '피스': 15,\n",
       "         '꼬임': 37,\n",
       "         '면불량': 9,\n",
       "         '창틀,문틀수정': 4,\n",
       "         '틈새과다': 1,\n",
       "         '이음부불량': 2,\n",
       "         '울음': 1})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(submit_test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f95f89d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LEARNING_RATE': 0.0003,\n",
       " 'IMG_SIZE': 224,\n",
       " 'architecture': 'Efficientnet-b0',\n",
       " 'EPOCHS': 10,\n",
       " 'SEED': 42,\n",
       " 'BATCH_SIZE': 32,\n",
       " 'dataset': 'Dacon-papering-competition'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "42c98108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model information 저장\n",
    "import json\n",
    "model_info = dict(CFG)\n",
    "with open(os.path.join(Record_path,'model_info.json'), 'w') as f:\n",
    "    json.dump(model_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6970a9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa05852a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7aa453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69965500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c7a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
